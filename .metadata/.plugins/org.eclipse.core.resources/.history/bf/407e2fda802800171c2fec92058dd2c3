'''
Created on Apr 23, 2017

@author: angel
3-5      DONE
6-8      DONE
9-10     DONE
11-13    DONE
14-16    easiest
17-19    HARD
20-22    HARDEST
23-25    EASIESTEST
'''
from books.datasci_from_scratch.ch_6_7_8.ch6_7_8 import minimize_stochastic

#Chapter 14: Simple Linear Regression

'''def predict(alpha, beta, x_i):
    return beta * x_i + alpha

def error(alpha, beta, x_i, y_i):
    return y_i - predict(alpha, beta, x_i)'''

def dot(v, w):    
    """v_1 * w_1 + ... + v_n * w_n"""    
    return sum(v_i * w_i               
               for v_i, w_i in zip(v, w))

def sum_of_squares(v):    
    """v_1 * v_1 + ... + v_n * v_n"""    
    return dot(v, v) 

from __future__ import division

def mean(x):    
    return sum(x) / len(x)


def de_mean(x):    
    """translate x by subtracting its mean (so the result has mean 0)"""    
    x_bar = mean(x)    
    return [x_i - x_bar for x_i in x]


def sum_of_squared_errors(alpha, beta, x, y):
    return sum(error(alpha, beta, x_i, y_i) ** 2
               for x_i, y_i in zip(x, y))

def variance(x):    
    """assumes x has at least two elements"""    
    n = len(x)    
    deviations = de_mean(x)    
    return sum_of_squares(deviations) / (n - 1)

import math

def standard_deviation(x):    
    return math.sqrt(variance(x))

def covariance(x, y):    
    n = len(x)    
    return dot(de_mean(x), de_mean(y)) / (n - 1)

def correlation(x, y):    
    stdev_x = standard_deviation(x)    
    stdev_y = standard_deviation(y)    
    if stdev_x > 0 and stdev_y > 0:        
        return covariance(x, y) / stdev_x / stdev_y    
    else:        
        return 0    # if no variation, correlation is zero

def least_squares_fit(x, y):
    beta = correlation(x, y) * standard_deviation(y) / standard_deviation(x)
    alpha = mean(y) - beta * mean(x)
    return alpha, beta

def total_sum_of_squares(y):
    return sum(v ** 2 for v in de_mean(y))

def r_squared(alpha, beta, x, y):
    return 1.0 - (sum_of_squared_errors(alpha, beta, x, y) / 
                  total_sum_of_squares(y))

'''def squared_error(x_i, y_i, theta):
    alpha, beta = theta
    return error(alpha, beta, x_i, y_i) ** 2

def squared_error_gradient(x_i, y_i, theta):
    alpha, beta = theta
    return [-2 * error(alpha, beta, x_i, y_i), #alpha partial derivative
            -2 * error(alpha, beta, x_i, y_i) * x_i] #beta partial derivative'''

#chapter 15 MULTIPLE REGRESSION



def predict(x_i, beta):
    return dot(x_i, beta)

def error(x_i, y_i, beta):
    return y_i - predict(x_i, beta)

def squared_error(x_i, y_i, beta):
    return error(x_i, y_i, beta) ** 2

def squared_error_gradient(x_i, y_i, beta):
    return [-2 * x_ij * error(x_i, y_i, beta)
            for x_ij in x_i]

